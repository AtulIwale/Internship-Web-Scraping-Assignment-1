{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28f16442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Header\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch the HTML content of the webpage\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all the header tags\n",
    "header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "# Extract the text from the header tags\n",
    "header_texts = [tag.get_text() for tag in header_tags]\n",
    "\n",
    "# Create a DataFrame from the header texts\n",
    "df = pd.DataFrame({'Header': header_texts})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92ea0e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Name  \\\n",
      "0           Shri Ram Nath Kovind (birth - 1945)   \n",
      "1             Shri Pranab Mukherjee (1935-2020)   \n",
      "2   Smt Pratibha Devisingh Patil (birth - 1934)   \n",
      "3            DR. A.P.J. Abdul Kalam (1931-2015)   \n",
      "4            Shri K. R. Narayanan (1920 - 2005)   \n",
      "5           Dr Shankar Dayal Sharma (1918-1999)   \n",
      "6               Shri R Venkataraman (1910-2009)   \n",
      "7                  Giani Zail Singh (1916-1994)   \n",
      "8         Shri Neelam Sanjiva Reddy (1913-1996)   \n",
      "9          Dr. Fakhruddin Ali Ahmed (1905-1977)   \n",
      "10     Shri Varahagiri Venkata Giri (1894-1980)   \n",
      "11                 Dr. Zakir Husain (1897-1969)   \n",
      "12     Dr. Sarvepalli Radhakrishnan (1888-1975)   \n",
      "13             Dr. Rajendra Prasad (1884-1963)    \n",
      "\n",
      "                                       Term of Office  \n",
      "0                      25 July, 2017 to 25 July, 2022  \n",
      "1                      25 July, 2012 to 25 July, 2017  \n",
      "2                      25 July, 2007 to 25 July, 2012  \n",
      "3                      25 July, 2002 to 25 July, 2007  \n",
      "4                      25 July, 1997 to 25 July, 2002  \n",
      "5                      25 July, 1992 to 25 July, 1997  \n",
      "6                      25 July, 1987 to 25 July, 1992  \n",
      "7                      25 July, 1982 to 25 July, 1987  \n",
      "8                      25 July, 1977 to 25 July, 1982  \n",
      "9                24 August, 1974 to 11 February, 1977  \n",
      "10  3 May, 1969 to 20 July, 1969 and 24 August, 19...  \n",
      "11                        13 May, 1967 to 3 May, 1969  \n",
      "12                       13 May, 1962 to 13 May, 1967  \n",
      "13                   26 January, 1950 to 13 May, 1962  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch the HTML content of the webpage\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Parse the HTML code\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all the list items containing former presidents' information\n",
    "president_items = soup.find_all('li')\n",
    "\n",
    "# Initialize lists to store the extracted data\n",
    "names = []\n",
    "terms_of_office = []\n",
    "\n",
    "# Extract the name and term of office for each former president\n",
    "for item in president_items:\n",
    "    name_element = item.find('h3')\n",
    "    term_element = item.find('span', class_='terms')\n",
    "    \n",
    "    # Check if the elements were found\n",
    "    if name_element is not None and term_element is not None:\n",
    "        name = name_element.text\n",
    "        term_of_office = term_element.next_sibling.strip()\n",
    "        names.append(name)\n",
    "        terms_of_office.append(term_of_office)\n",
    "\n",
    "# Create the DataFrame\n",
    "data = {'Name': names, 'Term of Office': terms_of_office}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70b3b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Team Matches Points Rating\n",
      "0     Australia      23   2714    118\n",
      "1      Pakistan      20   2316    116\n",
      "2         India      33   3807    115\n",
      "3   New Zealand      27   2806    104\n",
      "4       England      24   2426    101\n",
      "5  South Africa      19   1910    101\n",
      "6    Bangladesh      25   2451     98\n",
      "7   Afghanistan      10    878     88\n",
      "8     Sri Lanka      21   1682     80\n",
      "9   West Indies      25   1797     72\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the rankings\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "# Create empty lists to store the data\n",
    "teams = []\n",
    "matches = []\n",
    "points = []\n",
    "ratings = []\n",
    "\n",
    "# Extract the data for each team\n",
    "rows = table.find_all(\"tr\")\n",
    "for row in rows[1:11]:  # Extract top 10 teams\n",
    "    cells = row.find_all(\"td\")\n",
    "    team = cells[1].text.strip().split('\\n')[0]\n",
    "    match = cells[2].text.strip()\n",
    "    point = cells[3].text.strip().replace(\",\", \"\")\n",
    "    rating = cells[4].text.strip()\n",
    "    teams.append(team)\n",
    "    matches.append(match)\n",
    "    points.append(point)\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a DataFrame with the scraped data\n",
    "data = {\"Team\": teams, \"Matches\": matches, \"Points\": points, \"Rating\": ratings}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6db99322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Batsman Team Rating\n",
      "0             Babar Azam  PAK    886\n",
      "1  Rassie van der Dussen   SA    777\n",
      "2           Fakhar Zaman  PAK    755\n",
      "3            Imam-ul-Haq  PAK    745\n",
      "4           Shubman Gill  IND    738\n",
      "5           David Warner  AUS    726\n",
      "6           Harry Tector  IRE    722\n",
      "7            Virat Kohli  IND    719\n",
      "8        Quinton de Kock   SA    718\n",
      "9           Rohit Sharma  IND    707\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the rankings\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "# Create empty lists to store the data\n",
    "names = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "# Extract the data for each batsman\n",
    "rows = table.find_all(\"tr\")\n",
    "for row in rows[1:11]:  # Extract top 10 batsmen\n",
    "    cells = row.find_all(\"td\")\n",
    "    name = cells[1].text.strip()\n",
    "    team = cells[2].text.strip()\n",
    "    rating = cells[3].text.strip()\n",
    "    names.append(name)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a DataFrame with the scraped data\n",
    "data = {\"Batsman\": names, \"Team\": teams, \"Rating\": ratings}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5548ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Bowler Team                         Rating\n",
      "0    Josh Hazlewood  AUS      733 v England, 26/01/2018\n",
      "1    Mohammed Siraj  IND  736 v New Zealand, 21/01/2023\n",
      "2    Mitchell Starc  AUS  783 v New Zealand, 29/03/2015\n",
      "3        Matt Henry   NZ   691 v Bangladesh, 26/03/2021\n",
      "4       Trent Boult   NZ    775 v Australia, 11/09/2022\n",
      "5       Rashid Khan  AFG     806 v Pakistan, 21/09/2018\n",
      "6        Adam Zampa  AUS      655 v England, 22/11/2022\n",
      "7  Mujeeb Ur Rahman  AFG      712 v Ireland, 24/01/2021\n",
      "8     Mohammad Nabi  AFG     657 v Zimbabwe, 09/06/2022\n",
      "9    Shaheen Afridi  PAK  688 v West Indies, 10/06/2022\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the bowling rankings\n",
    "table = soup.find('table', class_='table')\n",
    "\n",
    "# Initialize lists to store the data\n",
    "bowlers = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "# Find all rows in the table except the header row\n",
    "rows = table.find_all('tr')[1:11]  # Extract top 10 bowlers\n",
    "\n",
    "# Iterate over the rows and extract the required data\n",
    "for row in rows:\n",
    "    # Find the columns in each row\n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    # Extract the bowler name, team, and rating\n",
    "    bowler = columns[1].text.strip()\n",
    "    team = columns[2].text.strip()\n",
    "    rating = columns[4].text.strip()\n",
    "    \n",
    "    # Append the data to the respective lists\n",
    "    bowlers.append(bowler)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a dictionary with the data\n",
    "data = {\n",
    "    'Bowler': bowlers,\n",
    "    'Team': teams,\n",
    "    'Rating': ratings\n",
    "}\n",
    "\n",
    "# Create a DataFrame using pandas\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c111777d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      21  3,603    172\n",
      "1      England\\nENG      28  3,342    119\n",
      "2  South Africa\\nSA      26  3,098    119\n",
      "3        India\\nIND      27  2,820    104\n",
      "4   New Zealand\\nNZ      25  2,553    102\n",
      "5   West Indies\\nWI      27  2,535     94\n",
      "6     Thailand\\nTHA      11    821     75\n",
      "7   Bangladesh\\nBAN      14    977     70\n",
      "8     Pakistan\\nPAK      27  1,678     62\n",
      "9     Sri Lanka\\nSL       9    479     53\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the team rankings\n",
    "table = soup.find('table', class_='table')\n",
    "\n",
    "# Initialize lists to store the data\n",
    "teams = []\n",
    "matches = []\n",
    "points = []\n",
    "ratings = []\n",
    "\n",
    "# Find all rows in the table except the header row\n",
    "rows = table.find_all('tr')[1:11]  # Extract top 10 teams\n",
    "\n",
    "# Iterate over the rows and extract the required data\n",
    "for row in rows:\n",
    "    # Find the columns in each row\n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    # Extract the team, matches, points, and rating\n",
    "    team = columns[1].text.strip()\n",
    "    match = columns[2].text.strip()\n",
    "    point = columns[3].text.strip()\n",
    "    rating = columns[4].text.strip()\n",
    "    \n",
    "    # Append the data to the respective lists\n",
    "    teams.append(team)\n",
    "    matches.append(match)\n",
    "    points.append(point)\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a dictionary with the data\n",
    "data = {\n",
    "    'Team': teams,\n",
    "    'Matches': matches,\n",
    "    'Points': points,\n",
    "    'Rating': ratings\n",
    "}\n",
    "\n",
    "# Create a DataFrame using pandas\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1ab99b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Player Team Rating\n",
      "0      Laura Wolvaardt   SA    732\n",
      "1       Natalie Sciver  ENG    731\n",
      "2          Meg Lanning  AUS    717\n",
      "3     Harmanpreet Kaur  IND    716\n",
      "4      Smriti Mandhana  IND    714\n",
      "5  Chamari Athapaththu   SL    673\n",
      "6         Ellyse Perry  AUS    626\n",
      "7       Tammy Beaumont  ENG    595\n",
      "8      Stafanie Taylor   WI    588\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the batting player rankings\n",
    "table = soup.find('table', class_='table')\n",
    "\n",
    "# Initialize lists to store the data\n",
    "players = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "# Find all rows in the table except the header row\n",
    "rows = table.find_all('tr')[1:11]  # Extract top 10 batting players\n",
    "\n",
    "# Iterate over the rows and extract the required data\n",
    "for row in rows:\n",
    "    # Find the columns in each row\n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    # Extract the player, team, and rating\n",
    "    player = columns[1].text.strip()\n",
    "    team = columns[2].text.strip()\n",
    "    rating = columns[3].text.strip()\n",
    "    \n",
    "    # Append the data to the respective lists\n",
    "    players.append(player)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a dictionary with the data\n",
    "data = {\n",
    "    'Player': players,\n",
    "    'Team': teams,\n",
    "    'Rating': ratings\n",
    "}\n",
    "\n",
    "# Create a DataFrame using pandas\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8910f5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Player Team Rating\n",
      "0    Hayley Matthews   WI    373\n",
      "1     Natalie Sciver  ENG    371\n",
      "2       Ellyse Perry  AUS    366\n",
      "3     Marizanne Kapp   SA    349\n",
      "4        Amelia Kerr   NZ    336\n",
      "5      Deepti Sharma  IND    322\n",
      "6   Ashleigh Gardner  AUS    292\n",
      "7      Jess Jonassen  AUS    250\n",
      "8           Nida Dar  PAK    232\n",
      "9  Sophie Ecclestone  ENG    205\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the player rankings\n",
    "table = soup.find('table', class_='table')\n",
    "\n",
    "# Initialize lists to store the data\n",
    "players = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "# Find all rows in the table except the header row\n",
    "rows = table.find_all('tr')[1:11]  # Extract top 10 all-rounders\n",
    "\n",
    "# Iterate over the rows and extract the required data\n",
    "for row in rows:\n",
    "    # Find the columns in each row\n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    # Extract the player name, team, and rating\n",
    "    player = columns[1].text.strip()\n",
    "    team = columns[2].text.strip()\n",
    "    rating = columns[3].text.strip()\n",
    "    \n",
    "    # Append the data to the respective lists\n",
    "    players.append(player)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a dictionary with the data\n",
    "data = {\n",
    "    'Player': players,\n",
    "    'Team': teams,\n",
    "    'Rating': ratings\n",
    "}\n",
    "\n",
    "# Create a DataFrame using pandas\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fa81d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline          Time  \\\n",
      "0   Turkey's President Erdogan seals election vict...   2 Hours Ago   \n",
      "1   Debt ceiling deal will be 'transformational' f...   4 Hours Ago   \n",
      "2   NYC, Las Vegas, D.C.: Free wellness activities...   6 Hours Ago   \n",
      "3   Content creators bring in up to $150/hour film...   6 Hours Ago   \n",
      "4   34-year-old makes up to $167 an hour nannying ...   7 Hours Ago   \n",
      "5   This winning fund puts a spin on emerging mark...   8 Hours Ago   \n",
      "6   ASCO will put the focus on the cancer fight. T...   8 Hours Ago   \n",
      "7   Now Boarding: Why airlines are turning to bigg...   8 Hours Ago   \n",
      "8   Global demand for streaming Asian movies, TV g...   8 Hours Ago   \n",
      "9   These are the cheapest tech stocks in the S&P 500   8 Hours Ago   \n",
      "10  Turkey votes in runoff election after candidat...  16 Hours Ago   \n",
      "11  White House and Republicans reach a tentative ...  19 Hours Ago   \n",
      "12  State Farm to stop accepting homeowners insura...  May 27, 2023   \n",
      "13  How Janie Deegan built Janie's Life-Changing B...  May 27, 2023   \n",
      "14  Top 10 cheapest places in the U.S. to buy a be...  May 27, 2023   \n",
      "15  Mark Cuban calls Elon Musk’s Twitter algorithm...  May 27, 2023   \n",
      "16  3 investing tips as the federal debt ceiling '...  May 27, 2023   \n",
      "17  Microsoft keyboard users are ‘devastated’ afte...  May 27, 2023   \n",
      "18  Steve Adcock: The 3 'stupidest' myths I've hea...  May 27, 2023   \n",
      "19  Chip stocks AMD and Nvidia are among the most ...  May 27, 2023   \n",
      "20  Analysts are pounding the table for these must...  May 27, 2023   \n",
      "21  Lower-income consumers pay for wealthy's credi...  May 27, 2023   \n",
      "22               How Mastercard has outperformed Visa  May 27, 2023   \n",
      "23  Why commercial real estate firms are joining t...  May 27, 2023   \n",
      "24  Japan stocks are on fire this year. Why the ra...  May 27, 2023   \n",
      "25  Marvell Technology shares surge after earnings...  May 26, 2023   \n",
      "26  The tech trade is back, driven by A.I. craze a...  May 26, 2023   \n",
      "27  Next week hints at only short-lived debt deal ...  May 26, 2023   \n",
      "28  Treasury says it could run out of money June 5...  May 26, 2023   \n",
      "29  Disney rips DeSantis bid to disqualify judge i...  May 26, 2023   \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.com/2023/05/28/turkeys-presid...  \n",
      "1   https://www.cnbc.com/2023/05/28/debt-ceiling-d...  \n",
      "2   https://www.cnbc.com/2023/05/28/nyc-las-vegas-...  \n",
      "3   https://www.cnbc.com/2023/05/28/content-creato...  \n",
      "4   https://www.cnbc.com/2023/05/28/gloria-richard...  \n",
      "5   https://www.cnbc.com/2023/05/28/this-winning-f...  \n",
      "6   https://www.cnbc.com/2023/05/28/asco-will-focu...  \n",
      "7   https://www.cnbc.com/2023/05/28/now-boarding-a...  \n",
      "8   https://www.cnbc.com/2023/05/28/squid-game-eea...  \n",
      "9   https://www.cnbc.com/2023/05/28/these-are-the-...  \n",
      "10  https://www.cnbc.com/2023/05/28/turkey-electio...  \n",
      "11  https://www.cnbc.com/2023/05/27/white-house-an...  \n",
      "12  https://www.cnbc.com/2023/05/27/state-farm-to-...  \n",
      "13  https://www.cnbc.com/2023/05/27/how-janie-deeg...  \n",
      "14  https://www.cnbc.com/2023/05/27/cheapest-place...  \n",
      "15  https://www.cnbc.com/2023/05/27/mark-cuban-say...  \n",
      "16  https://www.cnbc.com/2023/05/27/how-to-invest-...  \n",
      "17  https://www.cnbc.com/2023/05/27/microsoft-keyb...  \n",
      "18  https://www.cnbc.com/2023/05/27/steve-adcock-t...  \n",
      "19  https://www.cnbc.com/2023/05/27/chip-stocks-am...  \n",
      "20  https://www.cnbc.com/2023/05/27/analysts-are-p...  \n",
      "21  https://www.cnbc.com/2023/05/27/lower-income-a...  \n",
      "22  https://www.cnbc.com/2023/05/27/how-mastercard...  \n",
      "23  https://www.cnbc.com/2023/05/27/commercial-rea...  \n",
      "24  https://www.cnbc.com/2023/05/27/japan-stocks-a...  \n",
      "25  https://www.cnbc.com/2023/05/26/marvell-techno...  \n",
      "26  https://www.cnbc.com/2023/05/26/tech-stocks-ar...  \n",
      "27  https://www.cnbc.com/2023/05/26/next-week-hint...  \n",
      "28  https://www.cnbc.com/2023/05/26/treasury-says-...  \n",
      "29  https://www.cnbc.com/2023/05/26/disney-rips-de...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the container that holds the news articles\n",
    "container = soup.find(\"div\", class_=\"LatestNews-isHomePage\")\n",
    "\n",
    "# Create lists to store the news details\n",
    "headline_list = []\n",
    "time_list = []\n",
    "news_link_list = []\n",
    "\n",
    "# Extract the news details\n",
    "articles = container.find_all(\"li\", class_=\"LatestNews-item\")\n",
    "for article in articles:\n",
    "    headline = article.find(\"a\", class_=\"LatestNews-headline\").text.strip()\n",
    "    news_link = article.find(\"a\", class_=\"LatestNews-headline\")[\"href\"]\n",
    "    time = article.find(\"time\", class_=\"LatestNews-timestamp\").text.strip()\n",
    "\n",
    "    headline_list.append(headline)\n",
    "    time_list.append(time)\n",
    "    news_link_list.append(news_link)\n",
    "\n",
    "# Create a DataFrame with the scraped data\n",
    "data = {\n",
    "    \"Headline\": headline_list,\n",
    "    \"Time\": time_list,\n",
    "    \"News Link\": news_link_list\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da61bf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Paper Title  \\\n",
      "0                                    Reward is enough   \n",
      "1   Explanation in artificial intelligence: Insigh...   \n",
      "2              Creativity and artificial intelligence   \n",
      "3   Conflict-based search for optimal multi-agent ...   \n",
      "4   Knowledge graphs as tools for explainable mach...   \n",
      "5   Law and logic: A review from an argumentation ...   \n",
      "6   Between MDPs and semi-MDPs: A framework for te...   \n",
      "7   Explaining individual predictions when feature...   \n",
      "8       Multiple object tracking: A literature review   \n",
      "9   A survey of inverse reinforcement learning: Ch...   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   \n",
      "11  Explainable AI tools for legal reasoning about...   \n",
      "12            Hard choices in artificial intelligence   \n",
      "13  Assessing the communication gap between AI mod...   \n",
      "14  Explaining black-box classifiers using post-ho...   \n",
      "15  The Hanabi challenge: A new frontier for AI re...   \n",
      "16              Wrappers for feature subset selection   \n",
      "17  Artificial cognition for social human–robot in...   \n",
      "18  A review of possible effects of cognitive bias...   \n",
      "19  The multifaceted impact of Ada Lovelace in the...   \n",
      "20  Robot ethics: Mapping the issues for a mechani...   \n",
      "21          Reward (Mis)design for autonomous driving   \n",
      "22  Planning and acting in partially observable st...   \n",
      "23  What do we want from Explainable Artificial In...   \n",
      "\n",
      "                                              Authors  Published Date  \\\n",
      "0   David Silver, Satinder Singh, Doina Precup, Ri...    October 2021   \n",
      "1                                          Tim Miller   February 2019   \n",
      "2                                   Margaret A. Boden     August 1998   \n",
      "3   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...   February 2015   \n",
      "4                      Ilaria Tiddi, Stefan Schlobach    January 2022   \n",
      "5                      Henry Prakken, Giovanni Sartor    October 2015   \n",
      "6     Richard S. Sutton, Doina Precup, Satinder Singh     August 1999   \n",
      "7           Kjersti Aas, Martin Jullum, Anders Løland  September 2021   \n",
      "8                Wenhan Luo, Junliang Xing and 4 more      April 2021   \n",
      "9                       Saurabh Arora, Prashant Doshi     August 2021   \n",
      "10  Jasper van der Waa, Elisabeth Nieuwburg, Anita...   February 2021   \n",
      "11  Joe Collenette, Katie Atkinson, Trevor Bench-C...      April 2023   \n",
      "12   Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz   November 2021   \n",
      "13  Oskar Wysocki, Jessica Katharine Davies and 5 ...      March 2023   \n",
      "14  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...        May 2021   \n",
      "15          Nolan Bard, Jakob N. Foerster and 13 more      March 2020   \n",
      "16                         Ron Kohavi, George H. John   December 1997   \n",
      "17      Séverin Lemaignan, Mathieu Warnier and 3 more       June 2017   \n",
      "18    Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz       June 2021   \n",
      "19                             Luigia Carlucci Aiello       June 2016   \n",
      "20             Patrick Lin, Keith Abney, George Bekey      April 2011   \n",
      "21     W. Bradley Knox, Alessandro Allievi and 3 more      March 2023   \n",
      "22  Leslie Pack Kaelbling, Michael L. Littman, Ant...        May 1998   \n",
      "23             Markus Langer, Daniel Oster and 6 more       July 2021   \n",
      "\n",
      "                                            Paper URL  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "articles = soup.find_all(\"li\", class_=\"sc-9zxyh7-1 sc-9zxyh7-2 kOEIEO hvoVxs\")\n",
    "\n",
    "data = []\n",
    "for article in articles:\n",
    "    title = article.find(\"h2\").text.strip()\n",
    "    authors = article.find(\"span\", class_=\"sc-1w3fpd7-0 dnCnAO\").text.strip()\n",
    "    published_date = article.find(\"span\", class_=\"sc-1thf9ly-2 dvggWt\").text.strip()\n",
    "    paper_url = article.find(\"a\")[\"href\"]\n",
    "    data.append({\"Paper Title\": title, \"Authors\": authors, \"Published Date\": published_date, \"Paper URL\": paper_url})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42d90177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Restaurant Name                               Cuisine  \\\n",
      "0                   Castle Barbeque                       Connaught Place   \n",
      "1                   Jungle Jamboree                             3CS Mall,   \n",
      "2                        Cafe Knosh  The Leela Ambience Convention Hotel,   \n",
      "3                 Castle's Barbeque                         Pacific Mall,   \n",
      "4              The Barbeque Company                     Gardens Galleria,   \n",
      "5                       India Grill                    Hilton Garden Inn,   \n",
      "6                    Delhi Barbeque               Taurus Sarovar Portico,   \n",
      "7  The Monarch - Bar Be Que Village           Indirapuram Habitat Centre,   \n",
      "8                 Indian Grill Room               Suncity Business Tower,   \n",
      "\n",
      "                                            Location Rating  \\\n",
      "0                     Connaught Place, Central Delhi      4   \n",
      "1             3CS Mall,Lajpat Nagar - 3, South Delhi    3.9   \n",
      "2  The Leela Ambience Convention Hotel,Shahdara, ...    4.3   \n",
      "3             Pacific Mall,Tagore Garden, West Delhi    3.9   \n",
      "4                 Gardens Galleria,Sector 38A, Noida    3.9   \n",
      "5               Hilton Garden Inn,Saket, South Delhi    3.9   \n",
      "6     Taurus Sarovar Portico,Mahipalpur, South Delhi    3.7   \n",
      "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad    3.8   \n",
      "8   Suncity Business Tower,Golf Course Road, Gurgaon    4.3   \n",
      "\n",
      "                                           Image URL  \n",
      "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "8  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants/buffet-special\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all restaurant cards\n",
    "restaurant_cards = soup.find_all(\"div\", class_=\"restnt-card\")\n",
    "\n",
    "# Initialize lists to store the extracted data\n",
    "restaurant_names = []\n",
    "cuisines = []\n",
    "locations = []\n",
    "ratings = []\n",
    "image_urls = []\n",
    "\n",
    "# Extract information from each restaurant card\n",
    "for card in restaurant_cards:\n",
    "    # Restaurant name\n",
    "    name = card.find(\"a\", class_=\"restnt-name\").text\n",
    "    restaurant_names.append(name)\n",
    "\n",
    "    # Cuisine\n",
    "    cuisine = card.find(\"a\", href=lambda href: href and \"/delhi-restaurants\" in href).text\n",
    "    cuisines.append(cuisine)\n",
    "\n",
    "    # Location\n",
    "    location = card.find(\"div\", class_=\"restnt-loc\").text.strip()\n",
    "    locations.append(location)\n",
    "\n",
    "    # Rating\n",
    "    rating = card.find(\"div\", class_=\"restnt-rating\").text\n",
    "    ratings.append(rating)\n",
    "\n",
    "    # Image URL\n",
    "    image_url = card.find(\"img\", class_=\"no-img\")[\"data-src\"]\n",
    "    image_urls.append(image_url)\n",
    "\n",
    "# Create a DataFrame using the extracted data\n",
    "data = {\n",
    "    \"Restaurant Name\": restaurant_names,\n",
    "    \"Cuisine\": cuisines,\n",
    "    \"Location\": locations,\n",
    "    \"Rating\": ratings,\n",
    "    \"Image URL\": image_urls,\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b9bc64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
